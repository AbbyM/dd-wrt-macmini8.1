diff --git a/include/spl/sys/kmem.h b/include/spl/sys/kmem.h
index 23461fde8..aad57c916 100644
--- a/include/spl/sys/kmem.h
+++ b/include/spl/sys/kmem.h
@@ -47,11 +47,12 @@ extern void strfree(char *str);
 #define	KM_SLEEP	0x0000	/* can block for memory; success guaranteed */
 #define	KM_NOSLEEP	0x0001	/* cannot block for memory; may fail */
 #define	KM_PUSHPAGE	0x0004	/* can block for memory; may use reserve */
+#define	KM_ONCE		0x0008	/* like KM_SLEEP but do not loop allocation */
 #define	KM_ZERO		0x1000	/* zero the allocation */
 #define	KM_VMEM		0x2000	/* caller is vmem_* wrapper */
 #define	KM_KVMEM	0x4000	/* caller is kvmem_* wrapper */
 
-#define	KM_PUBLIC_MASK	(KM_SLEEP | KM_NOSLEEP | KM_PUSHPAGE)
+#define	KM_PUBLIC_MASK	(KM_SLEEP | KM_NOSLEEP | KM_PUSHPAGE | KM_ONCE)
 
 static int spl_fstrans_check(void);
 
diff --git a/include/sys/zfs_context.h b/include/sys/zfs_context.h
index f4e4b69db..1e1cd8faa 100644
--- a/include/sys/zfs_context.h
+++ b/include/sys/zfs_context.h
@@ -389,6 +389,7 @@ void procfs_list_add(procfs_list_t *procfs_list, void *p);
 #define	KM_SLEEP		UMEM_NOFAIL
 #define	KM_PUSHPAGE		KM_SLEEP
 #define	KM_NOSLEEP		UMEM_DEFAULT
+#define	KM_ONCE			UMEM_DEFAULT
 #define	KM_NORMALPRI		0	/* not needed with UMEM_DEFAULT */
 #define	KMC_NODEBUG		UMC_NODEBUG
 #define	KMC_KMEM		0x0
diff --git a/module/spl/spl-kmem.c b/module/spl/spl-kmem.c
index 60249207e..c0e9aa0f4 100644
--- a/module/spl/spl-kmem.c
+++ b/module/spl/spl-kmem.c
@@ -152,7 +152,7 @@ spl_kvmalloc(size_t size, gfp_t flags)
 		    (size <= PAGE_SIZE << PAGE_ALLOC_COSTLY_ORDER))
 			kmalloc_flags |= __GFP_NORETRY;
 	}
-	ret = kmalloc(size, kmalloc_flags);
+	ret = kmalloc_node(size, kmalloc_flags, NUMA_NO_NODE);
 	if (ret || size <= PAGE_SIZE)
 		return (ret);
 	return (__vmalloc(size, flags | __GFP_HIGHMEM, PAGE_KERNEL));
@@ -178,7 +178,6 @@ inline void *
 spl_kmem_alloc_impl(size_t size, int flags, int node)
 {
 	gfp_t lflags = kmem_flags_convert(flags);
-	int use_vmem = 0;
 	void *ptr;
 
 	/*
@@ -215,7 +214,7 @@ spl_kmem_alloc_impl(size_t size, int flags, int node)
 		 * impact performance so frequently manipulating the virtual
 		 * address space is strongly discouraged.
 		 */
-		if ((size > spl_kmem_alloc_max) || use_vmem) {
+		if ((size > spl_kmem_alloc_max)) {
 			if (flags & KM_VMEM) {
 				ptr = __vmalloc(size, lflags | __GFP_HIGHMEM,
 				    PAGE_KERNEL);
@@ -224,24 +223,14 @@ spl_kmem_alloc_impl(size_t size, int flags, int node)
 			}
 		} else {
 			if (flags & KM_VMEM)
-				ptr = kmalloc_node(size,
-				    lflags | __GFP_NORETRY, node);
+				ptr = spl_kvmalloc(size, lflags);
 			else
 				ptr = kmalloc_node(size, lflags, node);
 		}
 
-		if (likely(ptr) || (flags & KM_NOSLEEP))
+		if (likely(ptr) || (flags & KM_NOSLEEP) || (flags & KM_ONCE))
 			return (ptr);
 
-		/*
-		 * For vmem_alloc() and vmem_zalloc() callers retry immediately
-		 * using __vmalloc() which is unlikely to fail.
-		 */
-		if ((flags & KM_VMEM) && (use_vmem == 0))  {
-			use_vmem = 1;
-			continue;
-		}
-
 		/*
 		 * Use cond_resched() instead of congestion_wait() to avoid
 		 * deadlocking systems where there are no block devices.
diff --git a/module/zstd/zstd.c b/module/zstd/zstd.c
index 43a5cf5b6..8aa4b2f23 100644
--- a/module/zstd/zstd.c
+++ b/module/zstd/zstd.c
@@ -568,7 +568,7 @@ static int zstd_meminit(void)
 	zstd_cache_size[i].kmem_size = P2ROUNDUP(ZSTD_estimateDCtxSize() +
 	    sizeof (struct zstd_kmem), PAGESIZE);
 	zstd_kmem_cache[i] = kmem_cache_create(zstd_cache_config[i].cache_name,
-	    zstd_cache_size[i].kmem_size, 0, NULL, NULL, NULL, NULL, NULL, 
+	    zstd_cache_size[i].kmem_size, 0, NULL, NULL, NULL, NULL, NULL,
 	    KMC_KVMEM);
 	zstd_cache_size[i].kmem_flags = zstd_cache_config[i].flags;
 
